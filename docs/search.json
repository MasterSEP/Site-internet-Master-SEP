[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Présentation du Master SEP",
    "section": "",
    "text": "Master Statistique pour l’Évaluations et Prévision (SEP) offre aux étudiants l’opportunité d’acquérir une formation sur deux ans dans les domaines de l’analyse économique quantitative et de l’aide à la décision. La dualité de cette formation, à la fois théorique et appliquée, permet d’acquérir des compétences reconnues tant dans le monde professionnel que dans le cadre de la préparation d’un doctorat.\n\nLe parcours type « Statistique pour l’Évaluation et Prévision » (SEP) forme des statisticiens économistes aptes au dialogue avec des non spécialistes, performants dans tout domaine statistique, sur tout support informatique, et particulièrement efficients sur les problématiques de l’évaluation économique et sociale, de la Data Science, du traitement de données massives (Big Data), du traitement de données marketing et de la gestion des risques.\nA l’issue de la formation SEP, les étudiants auront acquis et approfondi les compétences dans les domaines\n– quantitatifs : analyse des données, séries temporelles, apprentissage statistique, aspects de classification et de mise en place de scores, économétrie, géostatistique, data mining, modélisation mathématique, techniques quantitatives en évaluation, fondements des mathématiques financières, mesure des risques.\n– informatiques : maîtrise des logiciels dédiés à la statistique et à la modélisation tels que IBM-SPSS, SAS ou encore R et Python, qui sont assimilés en contexte grâce à leur utilisation systématique lors des enseignements de statistique et en mode projet. Les aspects de codage ne sont pas en reste : les macro sous Excel, la programmation en VB-VBA ainsi qu’en langage objet R et Python ou encore la syntaxe de SAS et d’IBM-SPSS, font l’objet d’un enseignement spécifique. Enfin les requêtes ACCESS et SQL Server sont elles aussi maîtrisées. Les logiciels Hadoop et Spark dédiés au Big Data sont également présentés ;\n– métiers : Data Science, évaluation, prospective, mesure et gestion des risques en particulier financiers, marketing, télécommunications, projets Big Data, économie du développement durable, bio-économie.\nLa formation est pluridisciplinaire : les enseignements sont dispensés par des économistes, des mathématiciens et des informaticiens, ainsi que des professionnels des secteurs d’applications.\nLes compétences acquises dans ce Master dépassent largement le cadre de la statistique appliquée et de l’économétrie, pour couvrir les champs de l’économie et de l’informatique.\nUn premier tiers des enseignements est dispensé par des probabilistes-statisticiens et informaticiens, un second par des économistes, le reste étant réalisé par des professionnels des méthodes quantitatives rompus aux demandes et restitutions « tout public ».\n\n\n\n\n\nNos futurs statisticiens sont confrontés, dès leur formation, à la multidisciplinarité et à la nécessité d’user d’un langage commun grâce au recrutement d’étudiants issus de cursus variés : scientifique (mathématiciens, probabilistes, ingénieurs pluridisciplinaires et informaticiens) et domaine tertiaire (économistes avec fort acquis mathématique, économètres, gestionnaires, géographes ou sociologues ayant de fortes appétences quantitatives).\nLa première année du master, les enseignements sont pour les 6/10ème en rapport avec leur cursus d’origine pour aboutir à une offre commune en 2nde année.\nEn première année et dans une moindre mesure, en seconde année, il y a une mutualisation d’un certain nombre d’enseignements avec les parcours : « Transition écologique, politiques publiques» et « Entrepreneuriat, Innovation et Bio-économie » de la mention Économie Appliquée, «Calcul Scientifique» de la mention Mathématiques et Applications, « Calcul Haute Performance, Simulation ».\n\nLa formation s’achève avec un stage long soutenu devant l’ensemble du corps enseignant et de la nouvelle promotion. La pédagogie mise en œuvre est fondée sur la réalisation de projets utilisant les acquis de plusieurs enseignements, et réalisés systématiquement en groupe.\nLa formation se déroule sur le campus Sciences et sur le campus Économie de l’Université de Reims.\nLe Master délivre le SAS Joint Certificate Program « SAS Programing & Data Analysis ».\nLe master SEP de l’université de Reims Champagne-Ardenne a été classé parmi les meilleurs masters au classement 2023 “Master Big Data & Data Sciences” de Eduniversal."
  },
  {
    "objectID": "Modalités.html",
    "href": "Modalités.html",
    "title": "Modalités d’admissions",
    "section": "",
    "text": "Pour le M1 : être titulaire d’une L3 ou équivalent (180 crédits ECTS).\nPour le M2 : être titulaire d’un M1 (240 crédits ECTS).\nMentions de Licence recommandées :\nLicence Economie et Gestion\nLicence Mathématiques et informatique appliquées en sciences humaines et sociales\nLicence Mathématiques appliquées et sciences sociales\nPrérequis recommandés :\nPour suivre cette formation dans de bonnes conditions il est recommandé de posséder des acquis avérés en mathématiques, probabilité et statistique. Appétence pour l’objet informatique. Capacité à travailler en groupe. Fortes compétences rédactionnelles et bonne capacité d’expression orale. Curiosité.\nLes acquis de la licence en économie, la licence économie et gestion, ou la licence MIASHS sont idéaux lorsque les enseignements relatifs aux prérequis obligatoires sont obtenus avec aisance.\nPossibilité d’entrée en seconde année d’étudiant.e.s d’autres filières (Licence de géographie, psychologie, etc.) ayant acquis les prérequis obligatoires et ayant un projet professionnel de double compétence."
  },
  {
    "objectID": "plan_acces.html",
    "href": "plan_acces.html",
    "title": "Plan d’accès",
    "section": "",
    "text": "Les cours sont dispensés dans les deux campus : Campus Croix Rouge à la faculté d’économie et Campus Moulin de la Housse à la faculté des sciences, vous trouvez ci-dessous les adresses :\n\nUniversité de Reims Champagne-Ardenne :\nCampus Croix-Rouge\nBâtiment 18\n57 rue Pierre Taittinger\n51100 Reims\n\nCampus Moulin de la Housse\nDépartement Maths/Info\n16 Chemin des Rouliers\n51100 Reims\n\n\n\nVenir à Reims\nPar la route :\nÀ la croisée des autoroutes :\nA4 – E50 (Paris –Strasbourg-Allemagne)\nA26 – E17 (Lille –Lyon-Méditerranée)\nA34 – E46 (Ardennes –Belgique)\n6 sorties desservent la ville\nVous pouvez établir votre itinéraire en suivant les liens suivants :\nwww.mappy.com\nwww.viamichelin.com\nPar le train :\nReims est à 45’ de Paris et 30’ de Roissy Charles De Gaulle par le TGV Est Européen :\n2 gares TGV :\n* Gare TGV Reims centre : 8 A/R pour Paris en 45’\n* Gare TGV Reims Champagne à Bezannes (à 5km du centre de Reims) : 3 A/R pour Paris en 40’\n9 interconnexions avec le réseau national (Roissy Charles De Gaulle – 30’, Marne la Vallée – 30’, Massy – 1h, Strasbourg –1h50, Nantes – 3h15, Rennes – 3h17, Bordeaux – 4h36, Londres – 4h10, Lille – 1h34)\nPour plus de renseignements, connectez-vous sur le site de la SNCF www.voyages-sncf.com\nPar avion :\nAéroport Roissy Charles De Gaulle : à 30’ en TGV\nAéroport Paris-Orly : 1h30\nAéroport Paris-Vatry : 1h en navette (http://www.parisvatry.com/spip.php?article54)\nLiens vers les sites des aéroports :\nwww.aeroportsdeparis.fr\nwww.parisvatry.com\n\n\nSe déplacer à Reims\nEn tramway ou en bus sur le réseau Citura : www.citura.fr\nLes Journées thématiques auront lieu à l’Université de Reims sur le Campus Croix-Rouge, 57 rue Pierre Taittinger. Descendre à l’arrêt « Campus Croix-Rouge » sur les lignes A ou B du tramway.\n\nPlan du Campus Croix-Rouge\nPlan tramway Reims"
  },
  {
    "objectID": "Organisation.html",
    "href": "Organisation.html",
    "title": "Organisation des enseignements",
    "section": "",
    "text": "La formation est pluridisciplinaire. Les enseignements sont dispensés à parts égales par des mathématiciens, des économistes et des professionnels du secteur.\nLa spécialité de Master SEP se compose de deux pèriodes.\nAu cours de la première (de début septembre à fin mars), les étudiants suivent un corpus de cours obligatoires :\n\nstatistique/probabilités : séries temporelles, économétrie, analyse des données, Data-Mining, géomarketing, scoring et business intelligence, statistique exploratoire et big data ;\ninformatique : SAS, SPSS, SPLUS ®, R, VB-VBA, ACCESS-SQL Server, Git, hadoop ;\ntransversal : anglais, séminaire de formation à la recherche, implication dans la vie associative.\nfinance et gestion des risques : modélisation, mathématiques financières, mesure et gestion des risques, modélisation stochastique en temps continu, techniques statistiques pour l’analyse des risques difficiles à modéliser, introduction au traitement de l’image ;\néconomie : analyse multi-critères, acteurs et politiques publiques, gestion du développement durable.\n\nTous les cours sont évalués par des tests et des projets.\nOutre ces cours, les étudiants ont également accès au DataCamp.\nDataCamp est un navigateur pour apprendre les sciences de données dans le confort avec les didacticiels vidéo et les défis de codage sur R, Python, statistiques et d’autres langages de programmation. Le master SEP offre un accès gratuit pendant six mois au DataCamp pour acquérir une expérience pratique des outils et des processus que les scientifiques des données utilisent quotidiennement. Le lien de DataCamp est disponible ici."
  },
  {
    "objectID": "Enseignements.html",
    "href": "Enseignements.html",
    "title": "Liste des enseignements",
    "section": "",
    "text": "Un programme détaillé des enseignements et des intervenants est disponible en cliquant ici.\n\nProgramme des enseignements du premier semestre\n\n\n\n\n\n\n\n\nintitulés U.E. et E.C.\nE.C.T.S./\nVolume horaire\n\n\n\n\ncoef.\n\n\n\n\nU.E.\nE.C.\nC.M.\n\n\nU.E. 0911 Disciplinaire\n6\n\n\n\nSESG0903 -Evaluation et calcul économiques\n\n1\n\n\nSESG0907 -Evaluation économique avancée\n\n2\n\n\nSESG0902 -Atelier de prospective\n\n1\n\n\nMA0971 –Econométrie\n\n2\n\n\nU.E.0912 Disciplinaire de différenciation\n6\n\n\n\nMA0989 -Fondements des probabilités et statistique et introduction à R\n\n1\n\n\nMA0973 -Géostatistique\n\n1\n\n\nMA0974 -Système d’Information Géographique (SIG)\n\n1\n\n\nSESG0904 -Analyse des données\n\n2\n\n\nMA0976 -Option 1, un E.C. à choisir parmi (*)\n\n1\n\n\nU.E.0913 Disciplinaire de différenciation\n6\n\n\n\nMA0975 -Option 2, un E.C. à choisir parmi (*)\n\n2\n\n\nMA0977 -Initiation à la recherche : Séminaires « recherche » ou projet terrain\n\n2\n\n\nMA0979 -Séries temporelles, Hadoop\n\n2\n\n\nU.E. 0914 Disciplinaire\n6\n\n\n\nMA 0980 -Tests statistiques avancés avec applications sous R\n\n2\n\n\nMA0981 -Data Mining\n\n1\n\n\nMA0982 –Scoring et Business Intelligence\n\n1\n\n\nMA0983 -SAS\n\n1\n\n\nMA0984 -Statistique exploratoire, big data\n\n1\n\n\nU.E. 0915 Compétences Transversales\n6\n\n\n\nMA0985 -Implication dans la vie associative universitaire (IVAU)\n\n1\n\n\nAN0913 -Langue\n\n1\n\n\nMA0986 -ACCESS, SQL Server\n\n1\n\n\nMA0987 -VB-VBA et Excel\n\n1\n\n\nSESG0906 -SPSS\n\n1\n\n\nMA0988 -Techniques de Recherche d’Emploi et de Stage (TRES)\n\n1\n\n\n\n\n\nCours d’approfondissement (options du premier semestre)\n\n\n\n\n\n\n\nCours d’approfondissement : options 1 et 2\nVolume horaire\n\n\n\n\nC.M.\nT.D.\n\n\nMA0941 -Modélisation mathématique (Master MMSI)\n16\n\n\nMA0990 -Mesure et gestion des risques\n20\n\n\nMA0944 -Introduction au traitement d’images (Master MMSI)\n10\n\n\nEEDD0901 -Analyse multicritères (master EEDD)\n15\n\n\nEEDD0902 -Acteurs et politiques publics (Master EEDD)\n20\n\n\nEEDD0903 -Gestion du développement durable (Masters EEDD et Logistique)\n25\n\n\nSESG0901 -Economie du Développement Durable (Master EEDD)\n20\n\n\nRMS0902 -Modélisation stochastique en temps continu (RMS)\n20\n\n\nRMS0901 -Techniques statistiques pour l’analyse des risques difficiles à modéliser (RMS)\n20\n\n\n\n\n\nEnseignements du seconde semestre\n\n\n\n\n\n\n\n\n\n\nintitulés U.E. et E.C.\nE.C.T.S./\nVolume horaire\n\n\n\n\ncoef.\n\n\n\n\nU.E.\nE.C.\nC.M.\n\n\nU.E.1016.1 Disciplinaire de différenciation : Majeure professionnelle\n30\n\n\n\nMA1071 -Méthodologie du mémoire de recherche, mémoire de recherche\n\n8\n\n\nMA1073 -Méthodologie du rapport de stage, stage (de 3 à 6 mois), rapport et soutenance\n\n22\n\n\nU.E.1016.2 Disciplinaire de différenciation : Majeure recherche\n\n\n\n\nMA1072 -Méthodologie du rapport de stage, stage (de 1 à 6 mois), rapport\n\n8\n\n\nMA1074 -Méthodologie du mémoire de recherche, mémoire de recherche, soutenance\n\n22"
  },
  {
    "objectID": "contacts.html",
    "href": "contacts.html",
    "title": "Contacts",
    "section": "",
    "text": "Pour toute demande d’information concernant le parcours du master SEP, contacter :"
  },
  {
    "objectID": "Enseignants.html",
    "href": "Enseignants.html",
    "title": "Liste des enseignants",
    "section": "",
    "text": "Un programme détaillé des enseignements et des intervenants est disponible en cliquant ici."
  },
  {
    "objectID": "Stage.html",
    "href": "Stage.html",
    "title": "Stages",
    "section": "",
    "text": "Notre programme de stages va au-delà de l’acquisition de compétences techniques. Il offre une occasion unique d’explorer vos passions, de développer un réseau professionnel, et de vous préparer à affronter les défis du monde réel. Chez nous, chaque stage est une chance de grandir, d’apprendre, et de transformer vos ambitions en réussites concrètes.\nLe master SEP propose un stage long de 4 à 6 mois pour la majeure professionnelle et de 1 à 6 mois pour la majeure recherche.\nLivret de stages :\nCurieux de voir les accomplissements antérieurs de nos étudiants ? Explorez notre livret de stages pour découvrir une pléthore de projets innovants, d’expériences captivantes et de succès professionnels. Ce livret, alimenté par les réalisations exceptionnelles de nos promotions précédentes, est une source d’inspiration et de motivation pour les étudiants actuels et futurs du Master SEP."
  },
  {
    "objectID": "parle_de_nous.html",
    "href": "parle_de_nous.html",
    "title": "On parle de nous !",
    "section": "",
    "text": "Article du magazine Challenge\nArticle du journal Le Monde"
  },
  {
    "objectID": "Débouchés.html",
    "href": "Débouchés.html",
    "title": "Débouchés",
    "section": "",
    "text": "Métiers\nLes métiers visés sont très variés :\n\nData analyst / Data scientist / Data miner\nStatisticien\n\n\n\nChargé d’études statistiques ou économiques\nGestionnaire de risques / Risk manager\nGéomarketeur\nEconomètre\nAnalyste financier\nMarketeur quantitatif.\n\n\nLa rémunération moyenne annuelle brute constatée pour le premier emploi est 32 K€ et le délai d’attente pour ce premier emploi est généralement inférieur à 6 mois.\n\n\n\nSecteurs d’activités\nLe master S.E.P. permet de s’insérer facilement dans des secteurs d’activités aussi divers que ceux :\n\ndes cabinets de conseil statistique et économique : prospective, marketing quantitatif, gestion des risques, géomarketing, audimat, etc…\ndes administrations : locales, nationales, internationales;\nde l’industrie et des services : banques, assurances, pharmacie, etc…"
  },
  {
    "objectID": "Formation.html",
    "href": "Formation.html",
    "title": "Formation",
    "section": "",
    "text": "Explorez le détail de nos enseignements et plongez-vous dans les sujets passionnants que propose notre programme :\n\nOrganisation des enseignements\n\n\nEnvisagez votre avenir professionnel en consultant la liste des métiers accessibles après l’obtention de notre diplôme :\n\nDébouchés du master\n\n\nUne inscription au master vous intéresse ? Si vous êtes prêt à franchir le pas, retrouvez ici toutes les informations relatives aux conditions d’admission et aux prérequis.\n\nModalités d’admissions\n\n\nVous vous posez des questions sur les possibilités de stage ? Renseignez vous sur le parcours des anciens du master avec la page ci-dessous :\n\nStages des anciens étudiants"
  },
  {
    "objectID": "service_logement.html",
    "href": "service_logement.html",
    "title": "Livret d’acceuil",
    "section": "",
    "text": "Le CROUS est la principale structure pour obtenir un logement en résidence universitaire. Mais la chatoyante ville de Reims à beaucoup de logements chez des particuliers comme chez des privés, qui ne demande qu’à accueillir des étudiants.\nPour les étudiants étrangers, les démarches sont d’autant plus difficiles du point de vue administratif. Le livret d’accueil ci-dessous, spécifiquement dédié aux étudiants étrangers, résume l’ensemble des informations nécessaires à leur intégration.\nLivret d’accueil 2022-2023"
  },
  {
    "objectID": "vie_etudiante.html",
    "href": "vie_etudiante.html",
    "title": "vie_etudiante",
    "section": "",
    "text": "Plans d’accès\nService logement"
  },
  {
    "objectID": "ressources.html",
    "href": "ressources.html",
    "title": "Ressources",
    "section": "",
    "text": "Outil bibliographique\nhttps://biblioinversee.shinyapps.io/application-biblio-inversee/\n\nOutil proposé par des étudiants du Master 2 SEP pour la constitution de bibliographie et l’analyse statistiques sur des documents\n\nEffectuer une recherche sur Google Scholar et récupérer l’identifiant de citation dans l’url.\nRentrer dans la barre de recherche et télécharger pour récupérer les données.\n\n\n\n\nData Camp\nUn Data Camp présentant les prérequis de la formation pour les parcours économique et mathématique est disponible ici.\n\n\nEnquête d’insertion professionnelle\nLes anciens étudiants de la promotion 2019 ont été invités à répondre à un questionnaire visant à enquêter sur leurs situations professionnelles 30 mois après l’obtention du diplôme.\nVous pouvez retrouver le détail de l’enquête en cliquant ici\n\n\nLivret d’accueil\n\n\n\n\n\nLe CROUS est la principale structure pour obtenir un logement en résidence universitaire. Mais la chatoyante ville de Reims à beaucoup de logements chez des particuliers comme chez des privés, qui ne demande qu’à accueillir des étudiants.\nPour les étudiants étrangers, les démarches sont d’autant plus difficiles du point de vue administratif. Le livret d’accueil ci-dessous, spécifiquement dédié aux étudiants étrangers, résume l’ensemble des informations nécessaires à leur intégration.\nLivret d’accueil 2023-2024\n\n\n“Cheatsheets”\nLes cheatsheets ont été soigneusement élaborées pour vous offrir des références rapides et utiles sur différents langages de programmation :\nCheatsheets - Posit\nCheatsheets - DataCamp\nNous espérons que ces ressources vous seront utiles dans votre parcours académique et professionnel. N’hésitez pas à explorer et à partager ces cheatsheets avec vos pairs."
  },
  {
    "objectID": "ressources.html#row-1",
    "href": "ressources.html#row-1",
    "title": "Ressources",
    "section": "row",
    "text": "row\n\nClassement WTA\nWomen’s Tennis Association\nWTA est la principale association organisant les compétitions féminines de tennis. Aujourd’hui, l’importance des tournois féminins est égale à celle des tournois masculins ce qui n’était pas le cas à la création de la WTA en 1973. De plus, elle classe, tout au long de la saison, l’ensemble des joueuses actives en fonction de leurs performances.\nIci, vous aurez un aperçu de la carrière de , en commençant par sa carrière en générale depuis les années 2000, puis un focus sur la saison de votre choix et enfin un focus sur le tournoi de votre choix."
  },
  {
    "objectID": "Actualites.html",
    "href": "Actualites.html",
    "title": "Actualités",
    "section": "",
    "text": "10 bases de données orientées machine learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Scientist : de quoi parle-t-on ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData visualisation : comment valoriser au mieux ses données ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDégradation de la qualité et l’accessibilité des données en 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft lance officiellement SQL Server 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSalaires 2023 : +19% d’augmentation pour les data scientists\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Actualites.html#running-code",
    "href": "Actualites.html#running-code",
    "title": "Actualités",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "Articles/Article_1.html",
    "href": "Articles/Article_1.html",
    "title": "10 bases de données orientées machine learning",
    "section": "",
    "text": "Bien que leurs approches et leurs capacités diffèrent, de nombreuses bases de données permettent de créer des modèles d’apprentissage automatique pour exploiter et donner du sens à des mines d’informations. Aux côtés des mastodontes Amazon, Google, Oracle et Microsoft des plateformes comme BlazingSQL, Brytlyt et Kinetica sortent du lot.\nDans un article d’octobre 2022 intitulé Comment choisir une plateforme d’apprentissage automatique cloud, le premier conseil de Martin Heller pour le choix d’une plateforme était le suivant : « Soyez proche de vos données ». Garder le code près des données est nécessaire pour maintenir une faible latence, car la vitesse de la lumière limite les vitesses de transmission. Après tout, le machine learning (ML) - en particulier le deep learning - a tendance à parcourir toutes vos données plusieurs fois. L’idéal, pour les très grands ensembles de données, est de construire le modèle là où les données résident déjà, de sorte qu’aucune transmission massive de données ne soit nécessaire. Plusieurs bases de données le permettent dans une certaine mesure. La question suivante est naturellement : quelles bases de données prennent en charge le ML interne, et comment le font-elles ? Voici un tour d’horizon de ces bases de données par ordre alphabétique."
  },
  {
    "objectID": "Articles/Article_1.html#amazon-redshift",
    "href": "Articles/Article_1.html#amazon-redshift",
    "title": "10 bases de données orientées machine learning",
    "section": "Amazon Redshift",
    "text": "Amazon Redshift\nAmazon Redshift est un service managé d’entrepôt de données à l’échelle du pétaoctet, conçu pour rendre simple et rentable l’analyse de toutes les données à l’aide d’outils de veille stratégique existants. Il est optimisé pour des ensembles de données allant de quelques centaines de gigaoctets à un pétaoctet ou plus et coûte moins de 1 000 $ par téraoctet et par an. Redshift ML est conçu pour permettre aux utilisateurs de SQL de créer, former et déployer facilement des modèles de ML à l’aide de commandes SQL. CREATE MODEL de Redshift SQL définit par exemple les données à utiliser pour la formation et la colonne cible, puis les transmet à SageMaker Autopilot pour la formation via un bucket Amazon S3 chiffré dans la même zone.\n\nFonctionnement de Redshift ML. (Crédit : AWS)\nAprès l’entraînement AutoML, Redshift ML compile le meilleur modèle et l’enregistre en tant que fonction SQL de prédiction dans votre cluster Redshift. Vous pouvez ensuite invoquer le modèle pour l’inférence en appelant la fonction de prédiction dans une instruction SELECT. En résumé, Redshift ML utilise SageMaker Autopilot pour créer automatiquement des modèles de prédiction à partir des données que vous spécifiez via une instruction SQL, qui sont extraites dans un bucket S3. La meilleure fonction de prédiction trouvée est enregistrée dans le cluster Redshift."
  },
  {
    "objectID": "Articles/Article_1.html#blazingsql",
    "href": "Articles/Article_1.html#blazingsql",
    "title": "10 bases de données orientées machine learning",
    "section": "BlazingSQL",
    "text": "BlazingSQL\nBlazingSQL est un moteur SQL accéléré par le GPU construit sur l’écosystème RAPIDS ; il existe sous forme de projet open source et de service payant. RAPIDS est une suite de bibliothèques logicielles et d’API open source, incubée par Nvidia, qui utilise CUDA et est basée sur le format de mémoire en colonnes Apache Arrow. CuDF, qui fait partie de RAPIDS, est une bibliothèque GPU DataFrame de type Pandas permettant de charger, de joindre, d’agréger, de filtrer et de manipuler des données. Dask, également open source, apporte de la mise à l’échelle des paquets Python sur plusieurs machines. Dask peut distribuer les données et les calculs sur plusieurs GPU, soit dans le même système, soit dans un cluster multi-nœuds. Dask s’intègre à RAPIDS cuDF, XGBoost et RAPIDS cuML pour l’analyse de données et l’apprentissage automatique accélérés par le GPU.\nAinsi, BlazingSQL peut exécuter des requêtes accélérées par le GPU sur des data lake dans Amazon S3, transmettre les DataFrames résultants à cuDF pour la manipulation des données, et enfin effectuer l’apprentissage automatique avec RAPIDS XGBoost et cuML, et l’apprentissage profond avec PyTorch et TensorFlow."
  },
  {
    "objectID": "Articles/Article_1.html#brytlyt",
    "href": "Articles/Article_1.html#brytlyt",
    "title": "10 bases de données orientées machine learning",
    "section": "Brytlyt",
    "text": "Brytlyt\nBrytlyt est une plateforme basée sur un navigateur qui permet de traiter de l’IA dans une base de données avec des capacités de deep learning. Brytlyt combine une base de données PostgreSQL, PyTorch, Jupyter Notebooks, Scikit-learn, NumPy, Pandas et MLflow en une seule plateforme serverless qui sert de trois produits accélérés par GPU : une base de données, un outil de visualisation de données et un outil de science des données qui utilise des notebooks. Brytlyt se connecte à tout produit disposant d’un connecteur PostgreSQL, y compris les outils de BI tels que Tableau, et Python. Il supporte le chargement et l’ingestion de données à partir de fichiers de données externes tels que les CSV et à partir de sources de données SQL externes supportées par les wrappers de données étrangères (FDW) de PostgreSQL. Parmi ces dernières, citons Snowflake, Microsoft SQL Server, Google Cloud BigQuery, Databricks, Amazon Redshift et Amazon Athena.\nEn tant que base de données GPU avec traitement parallèle des jointures, Brytlyt peut traiter des milliards de lignes de données en quelques secondes. Brytlyt a des applications dans les télécommunications, le commerce de détail, le pétrole et le gaz, la finance, la logistique, l’ADN et la génomique. Avec PyTorch et Scikit-learn intégrés, Brytlyt peut supporter à la fois le deep learning et les modèles simples de ML fonctionnant en interne contre ses données. La prise en charge des GPU et le traitement parallèle signifient que toutes les opérations sont relativement rapides, même si l’entraînement de modèles d’apprentissage profond complexes sur des milliards de lignes prendra bien sûr un certain temps."
  },
  {
    "objectID": "Articles/Article_1.html#google-cloud-bigquery",
    "href": "Articles/Article_1.html#google-cloud-bigquery",
    "title": "10 bases de données orientées machine learning",
    "section": "Google Cloud BigQuery",
    "text": "Google Cloud BigQuery\nBigQuery est le datawarehouse infoféré de Google Cloud, à l’échelle du pétaoctet, pour exécuter des analyses sur de grandes quantités de données en temps quasi réel. BigQuery ML est utilisé pour créer et exécuter des modèles de ML dans BigQuery à l’aide de requêtes SQL. Ce data warehouse prend en charge la régression linéaire pour les prévisions, la régression logistique binaire et multi-classes pour la classification, le clustering K-means pour la segmentation des données, la factorisation matricielle pour la création de systèmes de recommandation de produits, les séries temporelles pour les prévisions de séries temporelles, y compris les anomalies, la saisonnalité et les jours fériés, les modèles de classification et de régression XGBoost, les réseaux neuronaux profonds basés sur TensorFlow pour les modèles de classification et de régression, les tableaux AutoML et l’importation de modèles TensorFlow.\n\nGoogle propose de faciliter les analyses en rassemblant les données provenant de plusieurs sources dans BigQuery. (Crédit : Google)\nVous pouvez utiliser un modèle avec des données provenant de plusieurs ensembles de données BigQuery pour l’entraînement et pour la prédiction. BigQuery ML n’extrait pas les données de l’entrepôt de données. Vous pouvez effectuer une ingénierie des fonctionnalités avec BigQuery ML en utilisant la clause TRANSFORM dans votre instruction CREATE MODEL. Selon Martin Heller, BigQuery ML apporte une grande partie de la puissance de Google Cloud Machine Learning dans l’entrepôt de données BigQuery avec la syntaxe SQL, sans extraire les données de l’entrepôt de données."
  },
  {
    "objectID": "Articles/Article_1.html#ibm-db2-warehouse",
    "href": "Articles/Article_1.html#ibm-db2-warehouse",
    "title": "10 bases de données orientées machine learning",
    "section": "IBM Db2 Warehouse",
    "text": "IBM Db2 Warehouse\nIBM Db2 Warehouse on Cloud est un service de cloud public managé. Il est possible de configurer cet environnement sur site avec son propre matériel ou dans un cloud privé. En tant qu’entrepôt de données, il comprend des fonctionnalités telles que le traitement des données en mémoire et les tableaux en colonnes pour le traitement analytique en ligne. Sa technologie Netezza fournit un ensemble robuste d’analyses conçues pour amener efficacement la requête aux données. Une gamme de bibliothèques et de fonctions vous aide à obtenir les informations précises dont on peut avoir besoin.\n\nObtenez un aperçu rapide de votre historique d’utilisation du stockage et de l’activité de chargement à l’aide du tableau de bord de la console Web. (Crédit : IBM)\nDb2 Warehouse prend en charge le ML dans la base de données en Python, R et SQL. Le module IDAX contient des procédures analytiques stockées, notamment l’analyse de la variance, les règles d’association, la transformation des données, les arbres de décision, les mesures de diagnostic, la discrétisation et les moments, le regroupement K-means, les voisins les plus proches, la régression linéaire, la gestion des métadonnées, la classification naïve Bayes, l’analyse en composantes principales, les distributions de probabilités, l’échantillonnage aléatoire, les arbres de régression, les modèles et règles séquentiels, ainsi que les statistiques paramétriques et non paramétriques."
  },
  {
    "objectID": "Articles/Article_1.html#kinetica",
    "href": "Articles/Article_1.html#kinetica",
    "title": "10 bases de données orientées machine learning",
    "section": "Kinetica",
    "text": "Kinetica\nKinetica Streaming Data Warehouse combine l’analyse de données historiques et en continu avec l’intelligence de localisation et l’IA dans une seule plateforme, le tout accessible via API et SQL. Kinetica est une base de données très rapide, distribuée, en colonnes, privilégiant la mémoire et accélérée par les GPU, avec des fonctionnalités de filtrage, de visualisation et d’agrégation.\nKinetica intègre des modèles et des algorithmes d’apprentissage automatique avec vos données pour une analyse prédictive en temps réel à l’échelle. Il vous permet de rationaliser vos pipelines de données et le cycle de vie de vos analyses, modèles d’apprentissage automatique et ingénierie des données, et de calculer des fonctionnalités avec le streaming. Kinetica fournit une solution de cycle de vie complet pour l’apprentissage automatique accéléré par les GPU : des carnets Jupyter gérés, l’entraînement des modèles via RAPIDS, et le déploiement et l’inférence automatisés des modèles dans la plateforme Kinetica. Il s’agit d’une solution complète de cycle de vie dans la base de données pour le ML accéléré par les GPU, et peut calculer des caractéristiques à partir de données en continu."
  },
  {
    "objectID": "Articles/Article_1.html#microsoft-sql-server",
    "href": "Articles/Article_1.html#microsoft-sql-server",
    "title": "10 bases de données orientées machine learning",
    "section": "Microsoft SQL Server",
    "text": "Microsoft SQL Server\nLes services d’apprentissage automatique de Microsoft SQL Server prennent en charge R, Python, Java, la commande T-SQL PREDICT et la procédure stockée rx_Predict dans le SGBDR SQL Server, et SparkML dans les grappes de données Big Data de SQL Server. Dans les langages R et Python, Microsoft inclut plusieurs paquets et bibliothèques pour le ML. Vous pouvez stocker vos modèles formés dans la base de données ou en externe. Azure SQL Managed Instance prend en charge Machine Learning Services for Python et R en preview.\n\nPour lancer l’installation, il suffit de démarrer l’assistant de configuration de SQL Server, puis dans l’onglet Installation, sélectionner Nouvelle installation autonome de SQL Server ou ajouter des fonctionnalités à une installation existante. (Crédit : Microsoft)\nMicrosoft R dispose d’extensions qui lui permettent de traiter les données à partir du disque ainsi qu’en mémoire. SQL Server fournit un cadre d’extension pour que le code R, Python et Java puisse utiliser les données et les fonctions de SQL Server. Les clusters Big Data de SQL Server exécutent SQL Server, Spark et HDFS dans Kubernetes. Lorsque SQL Server appelle le code Python, il peut à son tour invoquer Azure Machine Learning, et enregistrer le modèle résultant dans la base de données pour l’utiliser dans les prédictions."
  },
  {
    "objectID": "Articles/Article_1.html#mindsdb",
    "href": "Articles/Article_1.html#mindsdb",
    "title": "10 bases de données orientées machine learning",
    "section": "MindsDB",
    "text": "MindsDB\nSi votre base de données ne prend pas encore en charge l’apprentissage automatique interne, il est probable que vous puissiez ajouter cette fonctionnalité en utilisant MindsDB, qui s’intègre à une demi-douzaine de bases de données et à cinq outils de BI. Les bases de données prises en charge sont MariaDB, MySQL, PostgreSQL, ClickHouse, Microsoft SQL Server et Snowflake. Une intégration avec MongoDB est en cours et des intégrations avec des bases de données en continu sont promises pour 2021. Les outils de BI pris en charge sont actuellement SAS, Qlik Sense, Microsoft Power BI, Looker et Domo.\nMindsDB propose AutoML, les tableaux d’IA et l’IA explicable (XAI). Vous pouvez invoquer la formation AutoML à partir de MindsDB Studio, d’une instruction SQL INSERT ou d’un appel API Python. La formation peut éventuellement utiliser les GPU et créer un modèle de série chronologique. Vous pouvez enregistrer le modèle sous forme de table de base de données et l’appeler à partir d’une instruction SQL SELECT sur le modèle enregistré, à partir de MindsDB Studio ou d’un appel API Python. Vous pouvez évaluer, expliquer et visualiser la qualité du modèle à partir de MindsDB Studio. Il est également possible de connecter MindsDB Studio et l’API Python à des sources de données locales et distantes. MindsDB fournit en outre un cadre d’apprentissage profond simplifié, Lightwood, qui fonctionne sur PyTorch. En clair, MindsDB apporte des fonctionnalités utiles de ML à un certain nombre de bases de données qui ne disposent pas d’une prise en charge intégrée de cette technologie.\n\nA l’aide de données historiques, la solution MindsDB aide à prédire l’avenir. (Crédit : MindsDB)\nUn nombre croissant de bases de données prennent en charge l’apprentissage automatique en interne. Le mécanisme exact varie, et certains sont plus performants que d’autres. Si vous avez tellement de données que vous devriez autrement ajuster des modèles sur un sous-ensemble échantillonné, alors l’une des huit bases de données énumérées ci-dessus - et d’autres avec l’aide de MindsDB - pourrait aider à construire des modèles à partir de l’ensemble des données sans générer de frais généraux importants pour l’exportation des données."
  },
  {
    "objectID": "Articles/Article_1.html#oracle-database",
    "href": "Articles/Article_1.html#oracle-database",
    "title": "10 bases de données orientées machine learning",
    "section": "Oracle Database",
    "text": "Oracle Database\nOracle Cloud Infrastructure (OCI) Data Science est une plateforme managée et serverless destinée aux équipes de science des données pour construire, former et gérer des modèles de ML à l’aide d’Oracle Cloud Infrastructure, notamment Autonomous Database et Autonomous Data Warehouse. Elle comprend des outils, des bibliothèques et des packages centrés sur Python développés par la communauté open source et la bibliothèque Oracle Accelerated Data Science (ADS), qui prend en charge le cycle de vie de bout en bout des modèles prédictifs : acquisition, profilage, préparation et visualisation des données ; ingénierie des caractéristiques ; formation au modèle (y compris Oracle AutoML) ; évaluation, explication et interprétation du modèle (y compris Oracle MLX) ; déploiement de modèles vers Oracle Functions.\nOCI Data Science s’intègre au reste de la pile Oracle Cloud Infrastructure, notamment aux fonctions, au flux de données, à l’entrepôt de données autonome et au stockage d’objets. Les modèles actuellement pris en charge incluent : Oracle AutoML, Keras, Scikit-learn, XGBoost, ADSTuner (réglage des hyperparamètres). A noter qu’ADS prend également en charge l’explicabilité de l’apprentissage automatique (MLX)."
  },
  {
    "objectID": "Articles/Article_1.html#vertica",
    "href": "Articles/Article_1.html#vertica",
    "title": "10 bases de données orientées machine learning",
    "section": "Vertica",
    "text": "Vertica\nVertica Analytics Platform est un entrepôt de données évolutif à stockage en colonnes. Il fonctionne en deux modes : Enterprise, qui stocke les données localement dans le système de fichiers des nœuds qui constituent la base de données, et EON, qui stocke les données de manière communautaire pour tous les nœuds de calcul.\nVertica utilise un traitement massivement parallèle pour traiter des pétaoctets de données, et effectue son apprentissage automatique interne avec le parallélisme des données. Il dispose de huit algorithmes intégrés pour la préparation des données, de trois algorithmes de régression, de quatre algorithmes de classification, de deux algorithmes de clustering, de plusieurs fonctions de gestion des modèles et de la possibilité d’importer des modèles TensorFlow et PMML formés ailleurs. Une fois que vous avez ajusté ou importé un modèle, vous pouvez l’utiliser pour la prédiction. Vertica permet également des extensions définies par l’utilisateur et programmées en C++, Java, Python ou R ainsi que la syntaxe SQL pour la formation et l’inférence.\nArticle rédigé par Anirban Ghoshal sur “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_2.html",
    "href": "Articles/Article_2.html",
    "title": "Microsoft lance officiellement SQL Server 2022",
    "section": "",
    "text": "Microsoft a rendu SQL Server 2022 en disponibilité générale. Plusieurs évolutions sont à noter, notamment une forte imbrication avec le cloud Azure.\nLa déclinaison 2022 du SGBD de Microsoft, SQL Server, passe en mode GA (disponibilité générale). La firme de Redmond l’a annoncé à l’occasion du PASS (Professional Association for SQL Server) Community Summit qui s’est tenu récemment à Seattle. Il succède à SQL Server 2019, sorti il y a un peu plus de trois ans.\nPlusieurs évolutions sont à noter dans la base de données avec une prédominance autour du cloud Azure. Ainsi, dans le cadre d’un PRA, SQL server basculera vers Azure SQL Managed Instance. Par ailleurs, il existe une intégration spécifique avec Azure Synapse - un service datawarehouse et d’analyse de données qui comprend Apache Spark - et Azure Purview, pour la classification et la protection des données. Toujours sur ce dernier point, SQL Server 2022 supporte l’API AWS S3, également prise en charge par d’autres fournisseurs de stockage. Les utilisateurs peuvent ainsi élaborer des scénarios de sauvegarde et de restauration vers S3.\nUne autre fonctionnalité liée au cloud est un modèle optionnel de facturation basé sur Azure Arc (plateforme de cloud hybride), qui fait désormais partie du processus de configuration de SQL Server 2022. Azure Arc est capable de gérer SQL Server depuis Azure, ainsi que d’utiliser des services Azure tels que l’analyse des logs et Azure defender. Les utilisateurs peuvent payer à l’heure, en augmentant la consommation lors des pics de charge et en la diminuant pendant les périodes creuses."
  },
  {
    "objectID": "Articles/Article_2.html#accélérer-les-requêtes-et-améliorer-le-langage-t-sql",
    "href": "Articles/Article_2.html#accélérer-les-requêtes-et-améliorer-le-langage-t-sql",
    "title": "Microsoft lance officiellement SQL Server 2022",
    "section": "Accélérer les requêtes et améliorer le langage T-SQL",
    "text": "Accélérer les requêtes et améliorer le langage T-SQL\nLes performances ont été améliorées comme par exemple T-SQL, le langage de requête de SQL Server. Il comprend des fonctions supplémentaires autour de JSON (JavaScript Object Notation), des manipulations de bits comme LEFT_SHIFT et GET_BIT ou des séries chronologiques. Par ailleurs, il intègre une nouvelle expression IS DISTINCT FROM qui simplifie le traitement des valeurs nulles dans les expressions booléennes.\nL’optimisation des requêtes est également au rendez-vous avec Query Store. La fonction, qui capture l’historique des requêtes et ajuste les performances, est désormais activée par défaut. Elle était auparavant désactivée en raison d’un léger impact sur les performances. A travers ces différentes évolutions et améliorations, Microsoft espère consolider sa base installée face à une concurrence de plus en plus forte dans le domaine des bases de données notamment par les acteurs cloud comme AWS ou Google Cloud.\nArticle rédigé par Jacques Cheminat, rédacteur en chef, sur “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_3.html",
    "href": "Articles/Article_3.html",
    "title": "Les technologies en IA et analyse de données loin de leur plein potentiel",
    "section": "",
    "text": "En France, 63% des managers utilisent des technologies d’intelligence artificielle et d’analyse des données pour assurer leurs missions, selon une étude réalisée par Axys Consultants. Mais plusieurs facteurs, comme le manque de formation ou d’acculturation à ces outils les empêchent d’en tirer pleinement parti.\nDans l’Hexagone, les technologies d’IA et d’analyse des données sont les plus utilisées dans le cadre de la fonction managériale mais des freins au développement de ces dernières sont ressentis plus vivement. C’est le principal enseignement d’une étude nationale réalisée par Axys Consultants auprès de 220 cadres dirigeants. Les résultats montrent en effet que près de 2/3 des managers s’appuient sur ce type de solutions pour assurer leurs missions (63 %). Les outils big data (53 %) sont les plus utilisés par les cadres dirigeants, devant l’IA qui se place en 2ème position (43 %). Selon le cabinet, ces scores laissent supposer que les projets prévus en 2021 ont été menés à bien.  En effet, les répondants étaient 35 % à travailler sur le sujet et 9 % à avoir mis en œuvre des solutions concrètes faisant appel à l’IA (soit 44 %).\nLoin derrière on trouve la réalité virtuelle (12%), la réalité augmentée (6%) et le metaverse qui fait une timide percée à 2 %. A noter tout de même que plus d’un tiers des managers ne fait appel à aucune application citée dans ce classement. Le recours aux moteurs de recherche sémantique est en baisse par rapport à l’an dernier (25 % par rapport à 35 % en 2021), tout comme celui aux chatbots/voicebots (22 % contre 40 %). A l’inverse, les logiciels de reconnaissance à partir du texte ou de la voix, bien qu’encore peu usitées, sont 2 fois plus employées par les managers en 2022 (10 % contre 5 % en 2021)."
  },
  {
    "objectID": "Articles/Article_3.html#miser-sur-la-formation-en-réduisant-les-coûts",
    "href": "Articles/Article_3.html#miser-sur-la-formation-en-réduisant-les-coûts",
    "title": "Les technologies en IA et analyse de données loin de leur plein potentiel",
    "section": "Miser sur la formation en réduisant les coûts ",
    "text": "Miser sur la formation en réduisant les coûts \nMalgré l’intérêt croissant des directions générales pour ces solutions d’automatisation des tâches, certains freins sont évoqués de manière plus exacerbée quant à leur utilisation. Le premier handicap reste l’insuffisance de la formation et l’acculturation des utilisateurs à ces outils, un constat qui a plus que doublé en passant à 73 %en 2022 (contre 33 % l’an dernier). Pour preuve, plus de la moitié des décideurs (57 %) interrogés par Axys souhaitent renforcer leurs compétences dans les domaines de l’IA,  du machine et du deep learning et des data sciences. Ils sont également demandeurs de formation à la stratégie digitale (43 %). L’apprentissage de l’usage des outils collaboratifs dans le management est important pour plus d’un quart des répondants (27 %), tandis que l’acquisition de savoir-faire dans les méthodes agiles et le lean management et demandée par 20 % d’entre eux.\nAprès la montée en compétences, le deuxième point noir au développement de l’IA et des data dans les entreprises est le coût (cité par 55 % des répondants contre seulement 11 % en 2021). Parmi les autres freins, on trouve également la crainte de ne pas pouvoir expliquer les résultats de solutions comme l’IA, citée par 37% de sondés.  Les questions d’éthique qui étaient 3eme en 2021, rétrogradent à la 4ème place mais avec un pourcentage plus élevé (35 % contre 21 %). Si tous ces bémols ont augmenté en intensité, le seul qui échappe à cette règle est la crainte d’être remplacée par un robot qui est 2 fois moins prégnante (10 % contre 20 % en 2021)."
  },
  {
    "objectID": "Articles/Article_3.html#des-attentes-toujours-fortes-sur-lautomatisation",
    "href": "Articles/Article_3.html#des-attentes-toujours-fortes-sur-lautomatisation",
    "title": "Les technologies en IA et analyse de données loin de leur plein potentiel",
    "section": "Des attentes toujours fortes sur l’automatisation",
    "text": "Des attentes toujours fortes sur l’automatisation\nLe classement des attentes des cadres dirigeants vis-à-vis des technologies a également été modifié en 2022. Seule l’automatisation des tâches conserve sa 1ère place tout en affichant une très forte progression (67 % vs 44 %). Vient ensuite la possibilité d’améliorer l’action collective, citée par plus de la moitié des répondants (53 %). L’accroissement des performances remonte sur la troisième marche du podium avec un score qui a plus que quadruplé par rapport à 2021 à 51 %. Les items suivants affichent également de très fortes progressions par rapport à 2021. L’objectif d’utiliser la technologie / IA /big data pour aider le manager à prédire son activité et celle de ses équipes fait plus que tripler (41 % contre 16 %) et triple quasiment pour celui de mieux anticiper les risques (35 % contre 13 %).   \nArticle rédigé par Véronique Arène, journaliste, sur le “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_5.html",
    "href": "Articles/Article_5.html",
    "title": "Salaires 2023 : +19% d’augmentation pour les data scientists",
    "section": "",
    "text": "Comme chaque année, le cabinet de recrutement Robert Walters a publié son étude de rémunération pour 2023. Celle-ci prévoit une moyenne des augmentations salariales de 7% dans les métiers des data, des systèmes d’information et du marketing digital. En tête des plus fortes progressions, on trouve les data scientists de niveau expérimentés, suivis par les directeurs de projets IT.\nEn 2023, la rémunération restera le premier critère de satisfaction pour les cadres en France, notamment dans ce contexte inflationniste qui poussera 71% d’entre eux à demander une augmentation. Dans le secteur numérique, les salaires en 2022 étant élevées, cette tendance devrait s’accentuer en 2023. C’est ce qui ressort de la 24eme étude de rémunérations 2023 présentée par le cabinet Robert Walters ce mardi 13 décembre. Dans l’IT, des hausses de salaires sont attendues « notamment sur des profils d’analystes des données où les entreprises sont prêtes à s’aligner sur les demandes des meilleurs candidats », prévoit ce rapport.\nAinsi, il est prévu une moyenne des augmentations de7% en 2023 dans la catégorie des systèmes d’information et des data. En tête du palmarès, on trouve le data scientist avec 19% de hausse attendue l’an prochain sur le bulletin de salaire d’un profil ayant entre 5 et 10 ans d’expérience. Avec +14% de hausse sur leur fiche de paye, les directeurs de projet confirmés (avec 15 ans et plus d’expérience) ainsi que les « lead » data scientists ayant jusqu’à 10 ans d’expertise feront partie des mieux lotis, selon les projections du cabinet. Les raisons de cette surenchère salariale ? Une très forte demande des entreprises dans ces spécialités. Ainsi, en 2022, les offres d’emploi ont bondi de 77% dans le domaine des data par rapport à 2021 et de 62% pour les project managers."
  },
  {
    "objectID": "Articles/Article_5.html#des-offres-demploi-it-en-constante-progression",
    "href": "Articles/Article_5.html#des-offres-demploi-it-en-constante-progression",
    "title": "Salaires 2023 : +19% d’augmentation pour les data scientists",
    "section": "Des offres d’emploi IT en constante progression ",
    "text": "Des offres d’emploi IT en constante progression \n\nLa pénurie de profils IT a pour effet de faire progresser la moyenne des augmentations de salaires d’ici l’an prochain. (Source RobertWalters/Crédit image Robert Walters)\nA leurs côtés, les professionnels du marketing digital et des ventes dopés notamment par l’importance des stratégies commerciales numériques et le boom des achats en ligne devaient être valorisés de 7% en moyenne. Dans ces secteurs d’activité, deux postes se détachent en termes de progression salariale. Il s’agit du directeur e-commerce, soit +13% revalorisation pour un profil possédant 6 à 12 ans d’expérience et du directeur du digital (+9%) ayant exercé pendant 10 à 15 ans.\n\nDynamiques, les professions en marketing digital et e-commerce devraient être revalorisées en 2023.(Source RobertWalters/Crédit image Robert Walters)"
  },
  {
    "objectID": "Articles/Article_5.html#un-marché-porteur-malgré-le-contexte-économique",
    "href": "Articles/Article_5.html#un-marché-porteur-malgré-le-contexte-économique",
    "title": "Salaires 2023 : +19% d’augmentation pour les data scientists",
    "section": "Un marché porteur malgré le contexte économique",
    "text": "Un marché porteur malgré le contexte économique\nLes entreprises cherchent à constituer des équipes métiers et techniques où les expertises (data science, marketing digital, UX/UI, e- et m-commerce) complètent les fonctions traditionnelles (ERP, infrastructure, sécurité), fait remarquer cette étude. De même, les profils hybrides combinant technicité et approche business, capables de monter en puissance et de devancer une feuille de route IT & digitale en perpétuelle évolution seront également les plus prisés. En outre, au vu de l’explosion des cyberattaques les postes en cybersécurité deviennent une priorité pour les entreprises. Le lancement de projets de transformation vers le cloud, tout en intégrant la composante data, est aussi un enjeu fort à prendre en compte pour les entreprises.\nLe marché IT est totalement épargné par la situation économique pourtant préoccupante, souligne le cabinet de recrutement. La demande n’a jamais été aussi forte. Les candidats sont en position de force sur le marché : ils sont désormais difficiles à capter et restent réticents à tout changement.  Dans ce contexte les recruteurs ont compris qu’ils devraient faire des efforts pour attirer les meilleurs profils et semblent s’y adapter. \n\nMéthodologie :\nEtude de rémunération : données issues d’entretiens réalisés auprès de 50 000 candidats et clients dans le monde, de janvier à novembre 2022. Enquête Robert Walters : enquête réalisée auprès de plus de 1 700 cadres et entreprises interrogés en ligne en septembre 2022 en France.\nArticle rédigé par Véronique Arène, journaliste, sur “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_4.html",
    "href": "Articles/Article_4.html",
    "title": "Dégradation de la qualité et l’accessibilité des données en 2022",
    "section": "",
    "text": "D’après une étude publiée par l’éditeur Talend, la quasi-totalité des entreprises a éprouvé des difficultés à utiliser les données en 2022. En cause, une dégradation de la santé des données au cours de l’année passée, en particulier sur les aspects d’actualisation.\nPratiquement toutes les entreprises reconnaissent l’importance des données pour le succès de leurs stratégies, qu’il s’agisse d’augmenter le chiffre d’affaires, d’optimiser les coûts ou de réduire les risques. Pourtant, selon le baromètre 2022 sur la santé des données réalisé par l’éditeur Talend et la société Qualtrics, 97% des 892 décideurs et professionnels de la donnée interrogés témoignent de difficultés à utiliser les données dont ils disposent, leurs deux premiers défis étant d’assurer la qualité des données (49%) et d’y accéder rapidement (41%).\n\nÀ l’heure où l’agilité et la rapidité d’accès aux données peuvent faire la différence dans un contexte économique d’incertitude généralisée, près d’un sondé sur deux (46%) estime manquer de vitesse et de flexibilité dans ce domaine. Plus d’une entreprise sur quatre (41%) peine également à accéder rapidement aux données dont elle a besoin. L’enquête révèle par ailleurs une dégradation globale des cinq indicateurs de santé des données évalués entre 2021 et 2022 : l’actualisation chute de 18 points, la fiabilité, l’homogénéité et l’exhaustivité de 11 points et l’accessibilité de 9 points. Si malgré cela, 82% des répondants ont confiance dans leurs données, un écart notable sépare les professionnels IT (85% ont confiance) des métiers, qui ne sont que 75% à avoir confiance en celles-ci."
  },
  {
    "objectID": "Articles/Article_4.html#une-culture-data-encore-insuffisante",
    "href": "Articles/Article_4.html#une-culture-data-encore-insuffisante",
    "title": "Dégradation de la qualité et l’accessibilité des données en 2022",
    "section": "Une culture data encore insuffisante",
    "text": "Une culture data encore insuffisante\nParmi les facteurs expliquant la dégradation de la santé des données, les entreprises pointent notamment le travail à distance. Pour 57% des répondants, celui-ci accentue les difficultés d’accès à la donnée. Des enjeux d’ordre culturel transparaissent également, avec une organisation sur trois qui émet des réserves sur la compréhension des données par ses employés, faute en particulier de langage commun. Pour y remédier, 65% des entreprises interrogées ont lancé des programmes de data literacy (acculturation à la donnée).\nArticle rédigé par Aurélie Chandeze, rédactrice en chef adjointe de CIO, sur le “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Livret_acceuil.html",
    "href": "Livret_acceuil.html",
    "title": "Livret d’acceuil",
    "section": "",
    "text": "Pour les étudiants étrangers, les démarches sont d’autant plus difficiles du point de vue administratif. Le livret d’accueil ci-dessous, spécifiquement dédié aux étudiants étrangers, résume l’ensemble des informations nécessaires à leur intégration.\nLivret d’accueil 2022-2023"
  },
  {
    "objectID": "Livret_accueil.html",
    "href": "Livret_accueil.html",
    "title": "Livret d’accueil",
    "section": "",
<<<<<<< HEAD
    "text": "Le CROUS est la principale structure pour obtenir un logement en résidence universitaire. Mais la chatoyante ville de Reims à beaucoup de logements chez des particuliers comme chez des privés, qui ne demande qu’à accueillir des étudiants.\nPour les étudiants étrangers, les démarches sont d’autant plus difficiles du point de vue administratif. Le livret d’accueil ci-dessous, spécifiquement dédié aux étudiants étrangers, résume l’ensemble des informations nécessaires à leur intégration.\nLivret d’accueil 2023-2024"
  },
  {
    "objectID": "contacts.html#campus",
    "href": "contacts.html#campus",
    "title": "Contacts",
    "section": "Campus",
    "text": "Campus\nCampus Croix-Rouge\nBâtiment 18\n57 rue Pierre Taittinger\n51100 Reims"
=======
    "text": "Le CROUS est la principale structure pour obtenir un logement en résidence universitaire. Mais la chatoyante ville de Reims à beaucoup de logements chez des particuliers comme chez des privés, qui ne demande qu’à accueillir des étudiants.\nPour les étudiants étrangers, les démarches sont d’autant plus difficiles du point de vue administratif. Le livret d’accueil ci-dessous, spécifiquement dédié aux étudiants étrangers, résume l’ensemble des informations nécessaires à leur intégration.\nLivret d’accueil 2022-2023"
>>>>>>> 41627eae761b7d84ab0f125c099119c7d6c54c0b
  },
  {
    "objectID": "Programmes.html",
    "href": "Programmes.html",
    "title": "Programmes",
    "section": "",
<<<<<<< HEAD
    "text": "Un programme détaillé des enseignements et des intervenants est disponible en cliquant ici."
  },
  {
    "objectID": "docs/stage.html",
    "href": "docs/stage.html",
    "title": "Stages",
    "section": "",
    "text": "Le master SEP propose un stage long de 4 à 6 mois pour la majeure professionnelle et de 1 à 6 mois pour la majeure recherche.\n\nLivret des stages\nLe lien ci-dessous redirige vers un livret recensant les différents stages des promos précédentes du master SEP : livret_stage"
  },
  {
    "objectID": "Modalités.html#prérequis-obligatoires",
    "href": "Modalités.html#prérequis-obligatoires",
    "title": "Modalités d’admissions",
    "section": "",
    "text": "Pour le M1 : être titulaire d’une L3 ou équivalent (180 crédits ECTS).\nPour le M2 : être titulaire d’un M1 (240 crédits ECTS).\nMentions de Licence recommandées :\nLicence Economie et Gestion\nLicence Mathématiques et informatique appliquées en sciences humaines et sociales\nLicence Mathématiques appliquées et sciences sociales\nPrérequis recommandés :\nPour suivre cette formation dans de bonnes conditions il est recommandé de posséder des acquis avérés en mathématiques, probabilité et statistique. Appétence pour l’objet informatique. Capacité à travailler en groupe. Fortes compétences rédactionnelles et bonne capacité d’expression orale. Curiosité.\nLes acquis de la licence en économie, la licence économie et gestion, ou la licence MIASHS sont idéaux lorsque les enseignements relatifs aux prérequis obligatoires sont obtenus avec aisance.\nPossibilité d’entrée en seconde année d’étudiant.e.s d’autres filières (Licence de géographie, psychologie, etc.) ayant acquis les prérequis obligatoires et ayant un projet professionnel de double compétence."
  },
  {
    "objectID": "Modalités.html#admission",
    "href": "Modalités.html#admission",
    "title": "Modalités d’admissions",
    "section": "Admission",
    "text": "Admission\n\nNiveau à l’entrée en formation :\n\nniveau II (licence ou maîtrise universitaire)\n\nPour entrer en M1 :\n\nA partir du 1er février, les étudiants pourront consulter toutes les informations sur les diplômes nationaux de master sur le site :\nwww.monmaster.gouv.fr\n\nPour entrer en M2 :\n\nL’admission se fait en plusieurs étapes :\n\nCandidature en ligne : via le eCandidat de l’université de Reims\nDécision d’admissibilité pour les entretiens, sur la base du dossier de candidature.\nEntretien avec les candidats admissibles, puis décision d’admission.\nVoici les documents demandés :\n\n\n\n\n\n\n\nCurriculum Vitae\n\n\nLettre de motivation avec projet professionnel\n\n\nRelevé de notes de toutes vos années universitaires.\n\n\nCopies des diplômes obtenus (baccalauréat compris)\n\n\nAttestation(s) de stage\n\n\nAttestation(s) justifiant de l’expérience professionnelle en lien avec la formation\n\n\nAvis de l’Ambassade de France (Obligatoire pour les candidats non Européens résidant hors de l’Union Européenne)\n\n\nUn document avec le programme des études et le volume horaire des enseignements de chaque année d’études\n\n\n\n\nVous êtes de nationalité étrangère :\n\nLes modalités relatives à l’admission des étudiants étrangers sont disponibles sur le lien suivant : http://www.univ-reims.fr/etudiants-internationaux\nPour plus d’informations, vous pouvez également envoyer un e-mail : etudiants.etrangers@univ-reims.fr\nAdresse d’inscription : 2 avenue Robert Schuman"
  },
  {
    "objectID": "Article_6.html",
    "href": "Article_6.html",
    "title": "Data Scientist : de quoi parle-t-on ?",
    "section": "",
    "text": "Le métier de la Data Science (science des données) est un métier interdisciplinaire qui se fait de plus en plus connaître ces dernières années, mais sans en vraiment comprendre le sens. Data Scientist (scientifique des données) est d’ailleurs souvent confondu avec Data Analyst (analyste de données) et même certains professionnels du domaine parviennent difficilement à expliquer leur métier. Auparavant ces professionnels étaient même plutôt appelés des statisticiens !\n\n\n\n\n\nMais alors quelle est la différence et en quoi consiste le métier de Data Scientist ? Là où un Data Analyst ira interpréter les données pour en faire des rapports pour répondre à une question factuelle, un Data Scientist ira quant à lui plus en profondeur sur une question de prévision pour de l’aide à la décision. Pour mieux illustrer le propos, voici un exemple de problématique par métier :\n\nData Analyst : quelle a été la charge de travail du personnel tout au long des 12 derniers mois et quelles ont été les causes des éventuelles surcharges ?\nData Scientist : à partir de la charge de travail qu’a eue le personnel les 12 derniers mois et des causes des éventuelles surcharges, quelle sera cette charge pour le mois à venir et quelles pourraient être les causes d’une potentielle surcharge ?\n\nIci l’analyste va vous donner une interprétation de ce qu’il aura découvert et éventuellement une piste d’amélioration même s’il n’est pas certain que ce qu’il a découvert se reproduise, tandis que le scientifique va vous donner des prévisions de ce qu’il pourrait se produire et pourquoi, pour vous aider à l’anticiper.\n\n\n\n\n\nCependant un Data Scientist n’est pas devin ! Pour parvenir à réaliser ses prévisions, il doit d’abord passer par plusieurs étapes :\n1. L’analyse de la problématique et des données : oui, c’est une tâche qu’il a en commun avec l’analyste. A travers cette analyse il cherche à comprendre les données, comment les corriger lorsqu’elles présentent des incohérences ou des défauts, puis il cherche à savoir s’il existe des liens entre certaines données et ce qu’il souhaite prédire. Il peut également y ajouter des données externes, comme la météo, qui pourraient lui sembler pertinentes pour ses prévisions.\n2. L’analyse du métier qu’il étudie : là ce n’est pas son propre métier qu’il analyse mais celui de son interlocuteur. Afin de dégager des schémas pertinents et ne pas tomber dans le contreproductif, un Data Scientist a besoin de savoir un minimum comment vous travaillez et comment fonctionne votre structure. Par exemple, il pourrait vouloir comprendre quelles sont les possibilités d’un parcours patient lorsqu’il arrive aux urgences. Comprendre le métier et ses outils peut également l’aider à comprendre pourquoi parfois certaines données sont manquantes ou du moins incomplètes.\n3. La transformation des données : une fois toutes les informations réunies, il va transformer les données afin qu’elles soient utilisées correctement et produisent le moins d’erreur possible dans l’algorithme de prévision.\n4. La création de l’algorithme : l’étape la plus importante pour vous, c’est ici que la magie des mathématiques opère. A l’aide de plusieurs outils possibles, le Data Scientist va utiliser ou créer plusieurs modèles mathématiques et les tester les uns après les autres afin d’obtenir le meilleur résultat possible. Cette étape peut être longue puisque parfois il peut être amené à réadapter encore et encore ses paramètres selon les résultats obtenus lors des tests, ou bien même selon des recherches que lui-même ou d’autres auront menées en parallèle pour s’adapter aux progrès technologiques, comme en intelligence artificielle par exemple. Cette étape peut même aboutir à compléter les étapes précédentes au besoin. Une fois tout ce processus complété, le Data Scientist pourra finalement vous présenter ses meilleurs résultats. Ils ne seront pas toujours parfaits mais ils colleront au plus près de la réalité et vous donneront souvent une excellente piste pour faciliter les décisions !\n\n\n\n\n\nEt l’éthique du Data Scientist dans tout ça ? Elle englobe un ensemble de principes et de valeurs qui guident sa conduite dans la collecte, le traitement, l’analyse et l’utilisation des données. En raison de la nature souvent sensible des données et de l’impact potentiel sur la vie privée, l’éthique revêt une importance cruciale dans le domaine de la Data Science. Voici quelques principes éthiques auxquels un professionnel est généralement tenu de se conformer :\n- Respect de la vie privée : il doit prendre des mesures pour protéger la vie privée des individus dont les données sont utilisées dans leurs analyses. Cela inclut l’anonymisation des données lorsque cela est possible et le respect des réglementations en vigueur, telles que le Règlement Général sur la Protection des Données (RGPD) en Europe.\n- Transparence : il doit communiquer de manière transparente sur la manière dont les données sont collectées, traitées et utilisées. Cela implique de documenter les méthodes utilisées, de partager les sources de données, et d’expliquer les choix et les limites du modèle.\n- Consentement éclairé : Lorsque cela est applicable, obtenir le consentement éclairé des individus dont les données sont utilisées est une pratique éthique. Les personnes devraient être informées de la manière dont leurs données seront utilisées et avoir la possibilité de donner ou de refuser leur consentement.\n- Équité : le Data Scientist doit s’efforcer de garantir l’équité dans ses analyses. Cela signifie éviter les biais dans les données et les modèles, en s’assurant que les résultats ne discriminent pas certains groupes de personnes.\n- Sécurité des données : la protection des données contre l’accès non autorisé est une préoccupation majeure et c’est pourquoi il se doit de mettre en place des mesures de sécurité robustes pour prévenir les atteintes à la sécurité et garantir la confidentialité des données.\n- Responsabilité : le Data Scientist est responsable des conséquences de ses résultats. Il doit prendre en compte les implications éthiques de ses travaux et être prêt à assumer la responsabilité des résultats, en particulier s’ils ont un impact sur les individus ou la société.\n- Utilité sociale : il est encouragé à utiliser leurs compétences pour le bien de la société. Cela peut impliquer de contribuer à des projets qui ont un impact positif sur la santé, l’éducation, l’environnement, etc...\nIl est important de noter que les principes éthiques peuvent varier en fonction du contexte, de l’industrie et des réglementations locales. Les experts en Data Science sont souvent appelés à rester informés sur les meilleures pratiques et à s’adapter aux évolutions dans le domaine de l’éthique des données.\nArticle rédigé par une ancienne étudiante du master"
  },
  {
    "objectID": "Images/Article_7.html",
    "href": "Images/Article_7.html",
    "title": "Data visualisation : comment valoriser au mieux ses données ?",
    "section": "",
    "text": "La data visualisation joue un rôle crucial dans la création de valeur en rendant les données plus claires et faciles à comprendre. Alors que les organisations produisent de plus en plus de données, celles-ci ne sont pas toujours facilement interprétables. Afin de tirer pleinement parti du potentiel de ces données, il est essentiel d’utiliser des outils performants de data visualisation. Dans cet article, nous allons explorer ce qu’est la dataviz et comment l’utiliser pour valoriser vos données.\nRendant les données plus intelligibles et plus compréhensibles, la data visualisation favorise la création de valeur. \nSi les organisations génèrent de plus en plus de données, elles ne sont pas toujours intelligibles et compréhensibles. Alors pour exploiter pleinement tout le potentiel des données, il est primordial de se doter d’outils de data visualisations performantes. De quoi s’agit-il ? Comment utiliser la dataviz pour valoriser ses données ? C’est ce que nous allons voir dans cet article. \n\nQu’est-ce que la data visualisation ?\n\n\n\nDéfinition\n\nEntre les volumes de données exponentiels, les formats et les sources variées, les organisations disposent d’une grande diversité d’informations. Mais pour leur donner du sens, il est primordial de les rendre intelligibles et compréhensibles par l’ensemble des utilisateurs. C’est justement l’objectif de la data visualisation. Il s’agit de transformer des données brutes en informations claires et lisibles grâce à des représentations graphiques (tableaux de bord, des datas stories, camembert, etc). \nCe faisant, les utilisateurs (qu’ils soient expert data ou non) peuvent interpréter et analyser les données de manière intuitive. \nb. Bénéfices de la visualisation de données\nÀ l’heure du Big data, l’utilisation des données semble de plus en plus complexe. La data visualisation permet justement de simplifier cette exploitation :\n\nRendre les données intelligibles et facilement compréhensibles : grâce à la dataviz, la donnée n’est plus seulement une affaire de spécialistes. À travers les représentations visuelles, les utilisateurs sans compétences techniques peuvent comprendre simplement le message dégagé. \nPrendre de meilleures décisions : bien souvent, les données se présentent sous forme de tableaux avec des milliers de lignes. Or, parmi toutes ces informations disponibles, il est souvent difficile de dégager les éléments les plus importants. La data visualisation a justement pour but de mettre en valeur les insights les plus pertinents. Les organisations peuvent plus facilement identifier des opportunités, détecter des risques, résoudre des problèmes, et plus globalement atteindre leurs objectifs. \n\n\nComment valoriser ses données à travers des data visualisations ?\n\n\n\nChoisir une visualisation adaptée à vos données et au message que vous voulez transmettre\n\nPour prendre les meilleures décisions, les organisations ont tout intérêt à choisir les outils de data visualisation les plus pertinents. Voici quelques conseils : \n\nAdapter son message à l’audience : les données peuvent être destinées à des analystes data maîtrisant à la perfection les tableaux et les chiffres, mais aussi à des experts métiers sans aucune compétence technique. Alors pour rendre les informations intelligibles, il convient d’adapter la visualisation à l’utilisateur. \nChoisir les données les plus pertinentes : pour que votre data visualisation soit facilement compréhensible et exploitable, il est important de prioriser les données à utiliser et de s’assurer qu’elles sont de qualité. \nCroiser différents jeux de données : Pour créer des data visualisations, il est en général nécessaire de croiser vos données avec des données de référence (géographique, temporalité, etc). C’est également une bonne pratique pour dégager des tendances\nFaciliter l’accès aux data visualisation : Pour que vos data visualisation soient consultées et utilisées, il est indispensable d’en faciliter l’exploitation, en les réunissant sur un portail data unique par exemple. \n\nb. Contextualiser ses données à travers des dashboards ou data stories\nPour valoriser les différentes data visualisations créées à partir de vos données, il est important de les agréger sous la forme de : \n\nTableaux de bord : les dashboard sont utiles à tous les départements de l’organisation. En regroupant différentes data visualisations, il est ainsi possible de créer des tableaux de bord financiers, opérationnels ou des dashboards thématiques. \nDatastories : l’objectif est de raconter une histoire grâce à la donnée. On peut ainsi imaginer ajouter des images, des textes ou des liens et ressources externes pour donner plus de contexte aux données. \n\n\nConcevoir des data visualisations percutantes avec Opendatasoft\n\nEntre un renforcement du partage des données à l’ensemble des utilisateurs et une simplification des créations de data visualisation, l’accès aux données se démocratise progressivement. Toutes les parties prenantes peuvent ainsi interpréter les données à leur disposition, mais aussi transmettre les bons messages. Cela permet ainsi d’optimiser les opérations, de nourrir des relations de transparence avec les parties prenantes et d’accélérer le développement de son organisation.\nAvec la plateforme Opendatasoft, vous pouvez gérer tout le cycle de vie de vos données, de leurs enrichissement, en passant par la création de data visualisation et jusqu’au partage dans des formats adaptés à tous. \nEnrichir vos données pour construire des data visualisation pertinentes\nPour créer des data visualisation compréhensibles, il est indispensable d’utiliser des données complètes et de qualité. Opendatasoft fournit plus de 50 processeurs qui permettent aux utilisateurs de corriger ou de modifier leurs données sans avoir à écrire une seule ligne de code. Il est ainsi possible de : \n\nAjouter des données d’emplacement géographique ou appliquer des transformations géographiques. La plateforme permet même d’utiliser la géolocalisation à partir d’une adresse pour obtenir automatiquement un géopoint et une carte.\nNormaliser une date en adaptant vos données à la représentation standard de la date et de l’heure définie par la norme ISO 8601.\n\nLa plateforme Opendatasoft met à disposition un grand nombre de datasets de référence afin de faciliter le croisement des jeux de données et de permettre la création de nouveaux usages. \nSimplifier la création de data visualisations avec une solution no-code\nBien souvent, la data visualisation est la compétence exclusive d’expert de la donnée. Ces derniers maîtrisent à la perfection les langages de programmation, les solutions de business intelligence, etc. Or, les collaborateurs ne possédant pas de telles compétences sont limités quant à la valorisation des données. Comme ils ne peuvent créer simplement de data visualisation, ils n’exploitent pas pleinement les données à leur disposition. Cette exclusion des non-experts peut représenter un véritable manque à gagner pour les organisations, puisque les décisions ne sont pas prises en toute connaissance de cause. \nPour éviter cette situation, il est primordial de simplifier la création de data visualisation. Notamment grâce à des solutions no code.\nPartager ses dataviz en toute simplicité\nLa data visualisation ayant pour objectif de rendre les données compréhensibles par tous, il est primordial d’en favoriser le partage à l’ensemble des utilisateurs (aussi bien les collaborateurs en interne, que les partenaires externes, les fournisseurs, les clients, …). \nLa plateforme Opendatasoft facilite justement le partage de data viz avec toutes les parties prenantes.\nArticle rédigé par Silicon"
  },
  {
    "objectID": "Images/Alumni.html",
    "href": "Images/Alumni.html",
    "title": "Alumni",
    "section": "",
    "text": "PROMOTION 2023\n\n\n\nNom\nPrénom\n\n\n\n\nSZYMON\nSZPUNAR\n\n\nMAXIME\nLAINE\n\n\nOMAIMA\nGHENNAMI\n\n\nNOLAN\nPRUVOT\n\n\nGRACE MOLINGO\nMAMBIKA\n\n\nANTOINE\nQUEVILLART\n\n\nKOUSSO CLAVERIE\nKOFFI\n\n\nHAFSA\nSABONE\n\n\nHELENE\nNOVAKOWSKI\n\n\nKHAOULA\nAROUI\n\n\nNIHEL\nMEDJADJI\n\n\nMARTIN\nPERNEL\n\n\nOUSMANE\nFALL\n\n\nISMAEL DJOULDE\nDIALLO\n\n\nALIZEE\nSCHOLLHORN-LEOPOLD\n\n\nHOUDA\nBEL KORCHI\n\n\nEUSTACHE ADRIEN\nBIKEGNE\n\n\nYAHIA KASMI\nKASMI\n\n\nMARWA\nEL GHEMMAZ\n\n\nNOUHAILA\nKHOUNA\n\n\nYACINE\nZIDI"
  },
  {
    "objectID": "Images/Alumni.html#promotion-2023",
    "href": "Images/Alumni.html#promotion-2023",
    "title": "Alumni",
    "section": "",
    "text": "Nom\nPrénom"
=======
    "text": "MODALITÉS PÉDAGOGIQUES\nL’équipe enseignante utilise diverses méthodes et pratiques d’enseignement : le cours, les travaux dirigés, l’étude de cas, le projet, le mémoire, la présentation orale, la préparation aux entretiens…\nLe master est constitué de 3 semestres de cours, et un semestre de stage en fin de M2 avec rendu de mémoire.\n\n\nENSEIGNEMENTS\n\nMASTER 1 : SEP\n\n\n\n\n\n\n\n\nSEMESTRE 1\nVolume Horaire\nECTS\n\n\n\n\nProbabilités 1\n56h\n6\n\n\nAnalyse fonctionnelle 1\n28h\n3\n\n\nModélisation\n28h\n3\n\n\nModèles linéaires\n50h\n6\n\n\nOptimisation\n56h\n6\n\n\nAnglais de spécialité\n12h\n2\n\n\nAlgorithmique 1\n10h\n2\n\n\nSystème d’information géographique(SIG)-Geostatistique\n22h\n2\n\n\n\n\n\n\nSEMESTRE 2\nVolume Horaire\nECTS\n\n\n\n\nInférence statistique\n60h\n6\n\n\nIntroduction aux éléments finis\n60h\n6\n\n\nStatistique appliquée\n30h\n3\n\n\nMéthodes d’échantillonnage\n20h\n3\n\n\nProbabilités 2\n31h\n3\n\n\nAlgorithmique 2\n30h\n3\n\n\nMémoire de recherche/Stage\n2h\n6\n\n\n\n\n\n\n    \n    \n    Bouton de Redirection\n\n    \n\n\n  \n    Voir plus de détail sur le programme de M1\n\n    \n\n \n\n\n\nMASTER 2 : SEP\n\n\n\n\n\n\n\n\nSEMESTRE 3\nVolume Horaire\nECTS\n\n\n\n\nAnalyse des systèmes complexes\n50h\n6\n\n\nOutils big data\n25h\n3\n\n\nApprentissage automatique\n25h\n3\n\n\nAnalyse des données et data mining\n50h\n3\n\n\nApprentissage statistique et data mining\n50h\n3\n\n\nGestion des risques, séries temporelles, économétrie approfondie\n60h\n4\n\n\nTraitement et valorisation de données massives avec R et Rstudio\n40h\n2\n\n\nAnglais\n20h\n1\n\n\nImplication dans la vie universitaire\n13h\n1\n\n\nConférences, Gestion de projet & Projet Digital, SAS et VBA\n52h\n4\n\n\n\n\n\n\n\n\n\n\n\nSEMESTRE 4\nVolume Horaire\nECTS\n\n\n\n\nTechniques de recherche d’emploi et de stage (TRES), stage ou mémoire de recherche\n15h\n30\n\n\n\n\n\n\n    \n    \n    Bouton de Redirection\n\n    \n\n\n  \n    Voir plus de détail sur le programme de M2"
>>>>>>> 41627eae761b7d84ab0f125c099119c7d6c54c0b
  }
]